{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchsnooper\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from dataFunc import tensor_load, tensor_save\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载训练数据\n",
    "train_X = tensor_load('./dataset/train_X.npy')\n",
    "train_y = tensor_load('./dataset/train_y.npy')\n",
    "\n",
    "#加载验证数据\n",
    "valid_X = tensor_load('./dataset/valid_X.npy')\n",
    "valid_y = tensor_load('./dataset/valid_y.npy')\n",
    "\n",
    "#加载测试数据\n",
    "test_X = tensor_load('./dataset/test_X.npy')\n",
    "test_y = tensor_load('./dataset/test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch中需要保证label大于0\n",
    "train_y += 1\n",
    "valid_y += 1\n",
    "test_y += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X, valid_y = Variable(valid_X).long(), Variable(valid_y).long()\n",
    "test_X, test_y = Variable(test_X).long(), Variable(test_y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA:\n",
    "    train_X, train_y = train_X.cuda(), train_y.cuda()\n",
    "    valid_X, valid_y = valid_X.cuda(), valid_y.cuda()\n",
    "    test_X, test_y = test_X.cuda(), test_y.cuda()\n",
    "    \n",
    "\n",
    "#构建训练数据集\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "valid_dataset = TensorDataset(valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载词典\n",
    "with open('./dataset/vocab_dict.json', 'r') as f:\n",
    "    vocab_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, output_size, pad_idx, hidden_size, num_layers, bidirectional, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f1 = nn.Sequential(nn.Linear(hidden_size * 2, output_size),\n",
    "                                nn.Softmax())\n",
    "        \n",
    "    def forward(self, text):\n",
    "        x = self.embed(text) \n",
    "        x = self.dropout(x.permute(1,0,2)) #原词向量形状 (batch_size, seq_len, embedding_size)\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        hidden = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "        x = self.dropout(hidden.squeeze())\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, output_size, pad_idx, hidden_size, num_layers, bidirectional, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f1 = nn.Sequential(nn.Linear(hidden_size * 2, hidden_size),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.ReLU())\n",
    "        self.f2 = nn.Sequential(nn.Linear(hidden_size, output_size),\n",
    "                               nn.Softmax())\n",
    "        \n",
    "    def forward(self, text):\n",
    "        x = self.embed(text)  #原词向量形状 (batch_size, seq_len, embedding_size)   .permute(1,0,2)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.f1(x[:, -1, :])\n",
    "        return self.f2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_sizes, dropout):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                        nn.Conv2d(in_channels = 1, out_channels = num_filters, kernel_size = (fs, embedding_size))\n",
    "                        for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f1 = nn.Linear(num_filters * len(filter_sizes), output_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text.shape(batch_size, seq_len)\n",
    "        x = self.embed(text) # (batch_size, seq_len, embedding_size)\n",
    "        x = x.unsqueeze(1) #(batch_size, 1, seq_len, embedding_size)\n",
    "        convd = [F.relu(conv(x)).squeeze(3) for conv in self.convs] #(batch_size, num_filters, seq_len - filter_size + 1, 1)\n",
    "#         x = F.max_pool1d(x, x.shape[2])# (batch_size, num_filters, 1)\n",
    "#         x = x.squeeze(2)\n",
    "        x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in convd] # (batch_size, 3 * num_filters)\n",
    "        x = torch.cat(x, dim = 1)\n",
    "        x = self.dropout(x)\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, output_size,num_layers, hidden_size, bidirectional, \n",
    "                 pad_idx, num_filters, filter_sizes, dropout):\n",
    "        super(LSTM_CNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if bidirectional:\n",
    "        \n",
    "            self.convs = nn.ModuleList([\n",
    "                            nn.Conv2d(in_channels = 1, out_channels = num_filters, kernel_size = (fs, hidden_size * 2))\n",
    "                            for fs in filter_sizes\n",
    "            ])\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([\n",
    "                        nn.Conv2d(in_channels = 1, out_channels = num_filters, kernel_size = (fs, hidden_size))\n",
    "                        for fs in filter_sizes\n",
    "            ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f1 = nn.Linear(num_filters * len(filter_sizes), output_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text.shape(batch_size, seq_len)\n",
    "        x = self.embed(text) # (batch_size, seq_len, embedding_size)\n",
    "        \n",
    "        x = self.dropout(x.permute(1,0,2)) #原词向量形状 (batch_size, seq_len, embedding_size)\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        #hidden = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "        x = x.unsqueeze(1) #(seq_len, 1, batch_size, hidden_size * 双向（2）)\n",
    "        x = x.permute(2, 1, 0, 3)\n",
    "        convd = [F.relu(conv(x)).squeeze(3) for conv in self.convs] #(batch_size, num_filters, seq_len - filter_size + 1, 1)\n",
    "        \n",
    "        x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in convd] # (batch_size, 3 * num_filters)\n",
    "        x = torch.cat(x, dim = 1)\n",
    "        x = self.dropout(x)\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, num_layers, hidden_size, bidirectional, pad_idx,\n",
    "                 num_filters, filter_sizes, dropout):\n",
    "        super(CNN_LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                        nn.Conv2d(in_channels = 1, out_channels = num_filters * 2, kernel_size = (fs, embedding_size))\n",
    "                        for fs in filter_sizes\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f1 = nn.Sequential(nn.Linear(hidden_size * 2, output_size),\n",
    "                                nn.Softmax())\n",
    "        \n",
    "\n",
    "    def forward(self,text):\n",
    "        ## CNN\n",
    "        #text.shape(batch_size, seq_len)\n",
    "        x = self.embed(text)  # (batch_size, seq_len, embedding_size)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, seq_len, embedding_size)\n",
    "        convd = [F.relu(conv(x)).squeeze(3) for conv in\n",
    "                 self.convs]  # (batch_size, num_filters, seq_len - filter_size + 1, 1)\n",
    "        #         x = F.max_pool1d(x, x.shape[2])# (batch_size, num_filters, 1)\n",
    "        #         x = x.squeeze(2)\n",
    "        x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in convd]  # (batch_size, 3 * num_filters)\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        ## LSTM\n",
    "        x = x.unsqueeze(0)\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        hidden = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "        x = self.dropout(hidden.squeeze())\n",
    "\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正确率计算\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def accuracy_computing(preds, y):\n",
    "    rounded_preds = preds.argmax(dim = 1)\n",
    "    #print(rounded_preds.shape)\n",
    "    #print(y.shape)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    #print(rounded_preds)\n",
    "    #print(y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    MAP = f1_score(y.cpu().detach().numpy(), rounded_preds.cpu().detach().numpy(), average = 'macro', labels=[0, 1, 2])\n",
    "    return acc, MAP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参定义\n",
    "VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_SIZE = 128\n",
    "PAD_IDX = vocab_dict['<pad>'] # 就是<pad>的index\n",
    "UNK_IDX = vocab_dict['<unk>'] #<unk>的index\n",
    "\n",
    "OUTPUT_SIZE = 3 #输出结果为1个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "hidden_size = 128\n",
    "dropout = 0.5\n",
    "epoches = 30\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 0)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         shuffle = True,\n",
    "                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'lstm-cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型声明\n",
    "if METHOD == 'lstm':\n",
    "    model = LSTMModel(vocab_size = VOCAB_SIZE, \n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX,\n",
    "                     num_layers = num_layers,\n",
    "                     bidirectional = bidirectional,\n",
    "                     hidden_size = hidden_size,\n",
    "                     dropout = dropout)\n",
    "elif METHOD == 'gru':\n",
    "    model = GRUModel(vocab_size = VOCAB_SIZE, \n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX,\n",
    "                     num_layers = num_layers,\n",
    "                     bidirectional = bidirectional,\n",
    "                     hidden_size = hidden_size,\n",
    "                     dropout = dropout)\n",
    "elif METHOD == 'cnn':\n",
    "    model = CNNModel(vocab_size = VOCAB_SIZE, \n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX,\n",
    "                     num_filters = 100,\n",
    "                     filter_sizes = [3, 4, 5], \n",
    "                     dropout = dropout\n",
    "                    )\n",
    "    \n",
    "elif METHOD == 'lstm-cnn':\n",
    "    model = LSTM_CNNModel(vocab_size = VOCAB_SIZE, \n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX,\n",
    "                     num_layers = num_layers,\n",
    "                     bidirectional = bidirectional,\n",
    "                     hidden_size = hidden_size,\n",
    "                     dropout = dropout,\n",
    "                     num_filters = 64,\n",
    "                     filter_sizes = [3]\n",
    "                    )\n",
    "\n",
    "elif METHOD == 'cnn-lstm':\n",
    "    model = CNN_LSTMModel(vocab_size = VOCAB_SIZE, \n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX,\n",
    "                     num_layers = num_layers,\n",
    "                     bidirectional = bidirectional,\n",
    "                     hidden_size = hidden_size,\n",
    "                     dropout = dropout,\n",
    "                     num_filters = 32,\n",
    "                     filter_sizes = [3]\n",
    "                    )\n",
    "\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置损失函数与优化器\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "model = model.to(device)\n",
    "Loss = Loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单epoch训练函数\n",
    "#@torchsnooper.snoop()\n",
    "def train(model, Loss, optimizer, train_loader):\n",
    "    epoch_loss, epoch_acc ,epoch_MAP = 0., 0., 0.\n",
    "    total_len = 0\n",
    "    model.train()\n",
    "    for step, data in enumerate(train_loader):\n",
    "        start = time.clock()\n",
    "        batch_x, batch_y = data\n",
    "        #print(model.device)\n",
    "        # 将这些数据转换成Variable类型\n",
    "        batch_x, batch_y = Variable(batch_x).long(), Variable(batch_y).long()\n",
    "        output = model(batch_x)\n",
    "        \n",
    "        acc, MAP = accuracy_computing(output, batch_y.squeeze())\n",
    "        loss = Loss(output, batch_y.squeeze())\n",
    "        #SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch_y)\n",
    "        epoch_acc += acc.item() * len(batch_y)\n",
    "        epoch_MAP += MAP.item() * len(batch_y)\n",
    "        total_len += len(batch_y)\n",
    "        \n",
    "        \n",
    "        total_loss = epoch_loss / total_len\n",
    "        total_acc =  epoch_acc / total_len\n",
    "        total_MAP = epoch_MAP / total_len\n",
    "        end = time.clock()\n",
    "        #print('Step:', step, 'Train Loss: %0.6f'% loss.data, 'Train Accuracy: %0.4f'% acc, 'Train MAP:',MAP ,'Time: %0.6f'% (end-start))\n",
    "    return total_loss, total_acc, total_MAP\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型\n",
    "def evaluate(model, Loss, optimizer, valid_loader):\n",
    "    epoch_loss, epoch_acc ,epoch_MAP = 0., 0., 0.\n",
    "    total_len = 0\n",
    "    model.eval()\n",
    "    for step, data in enumerate(valid_loader):\n",
    "        batch_x, batch_y = data\n",
    "        # 将这些数据转换成Variable类型\n",
    "        batch_x, batch_y = Variable(batch_x).long(), Variable(batch_y).long()\n",
    "        output = model(batch_x)\n",
    "        \n",
    "        acc, MAP = accuracy_computing(output, batch_y.squeeze())\n",
    "        loss = Loss(output, batch_y.squeeze())\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch_y)\n",
    "        epoch_acc += acc.item() * len(batch_y)\n",
    "        epoch_MAP += MAP.item() * len(batch_y)\n",
    "        total_len += len(batch_y)\n",
    "        \n",
    "        total_loss = epoch_loss / total_len\n",
    "        total_acc =  epoch_acc / total_len\n",
    "        total_MAP = epoch_MAP / total_len\n",
    "    model.train()\n",
    "    return total_loss, total_acc, total_MAP\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行第0个epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingmuhe/conda/envs/begin/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n",
      "/home/dingmuhe/conda/envs/begin/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch: 0 Train Loss: 0.875899 Train MAP: 0.40507039459980215 Train Accuracy: 0.6220\n",
      "Epoch: 0 Valid Loss: 0.820566 Valid MAP: 0.48379780347217705 Valid Accuracy: 0.6432\n",
      "--------------------\n",
      "进行第1个epoch\n",
      "--------------------\n",
      "Epoch: 1 Train Loss: 0.833032 Train MAP: 0.45583077130979127 Train Accuracy: 0.6405\n",
      "Epoch: 1 Valid Loss: 0.803826 Valid MAP: 0.46309584989196384 Valid Accuracy: 0.6517\n",
      "--------------------\n",
      "进行第2个epoch\n",
      "--------------------\n",
      "Epoch: 2 Train Loss: 0.814974 Train MAP: 0.4684669702240651 Train Accuracy: 0.6486\n",
      "Epoch: 2 Valid Loss: 0.795683 Valid MAP: 0.47776044939475576 Valid Accuracy: 0.6582\n",
      "--------------------\n",
      "进行第3个epoch\n",
      "--------------------\n",
      "Epoch: 3 Train Loss: 0.799001 Train MAP: 0.4807225176395516 Train Accuracy: 0.6531\n",
      "Epoch: 3 Valid Loss: 0.785378 Valid MAP: 0.50230537986196 Valid Accuracy: 0.6624\n",
      "--------------------\n",
      "进行第4个epoch\n",
      "--------------------\n",
      "Epoch: 4 Train Loss: 0.785658 Train MAP: 0.4961435186693958 Train Accuracy: 0.6572\n",
      "Epoch: 4 Valid Loss: 0.776376 Valid MAP: 0.5387745179628007 Valid Accuracy: 0.6666\n",
      "--------------------\n",
      "进行第5个epoch\n",
      "--------------------\n",
      "Epoch: 5 Train Loss: 0.775948 Train MAP: 0.5060241192390609 Train Accuracy: 0.6627\n",
      "Epoch: 5 Valid Loss: 0.771206 Valid MAP: 0.5229976695903769 Valid Accuracy: 0.6674\n",
      "--------------------\n",
      "进行第6个epoch\n",
      "--------------------\n",
      "Epoch: 6 Train Loss: 0.766217 Train MAP: 0.5174104859413975 Train Accuracy: 0.6659\n",
      "Epoch: 6 Valid Loss: 0.764036 Valid MAP: 0.5290158411206358 Valid Accuracy: 0.6665\n",
      "--------------------\n",
      "进行第7个epoch\n",
      "--------------------\n",
      "Epoch: 7 Train Loss: 0.759036 Train MAP: 0.5261437204682013 Train Accuracy: 0.6694\n",
      "Epoch: 7 Valid Loss: 0.768085 Valid MAP: 0.5479628933452142 Valid Accuracy: 0.6673\n",
      "--------------------\n",
      "进行第8个epoch\n",
      "--------------------\n",
      "Epoch: 8 Train Loss: 0.749523 Train MAP: 0.5374723495852727 Train Accuracy: 0.6724\n",
      "Epoch: 8 Valid Loss: 0.760203 Valid MAP: 0.540924637596024 Valid Accuracy: 0.6695\n",
      "--------------------\n",
      "进行第9个epoch\n",
      "--------------------\n",
      "Epoch: 9 Train Loss: 0.743181 Train MAP: 0.5379473576429312 Train Accuracy: 0.6735\n",
      "Epoch: 9 Valid Loss: 0.767326 Valid MAP: 0.577107443555517 Valid Accuracy: 0.6745\n",
      "--------------------\n",
      "进行第10个epoch\n",
      "--------------------\n",
      "Epoch: 10 Train Loss: 0.735076 Train MAP: 0.5527549109354372 Train Accuracy: 0.6782\n",
      "Epoch: 10 Valid Loss: 0.756246 Valid MAP: 0.5601307780326651 Valid Accuracy: 0.6762\n",
      "--------------------\n",
      "进行第11个epoch\n",
      "--------------------\n",
      "Epoch: 11 Train Loss: 0.728649 Train MAP: 0.5607347239703349 Train Accuracy: 0.6819\n",
      "Epoch: 11 Valid Loss: 0.756314 Valid MAP: 0.5661725970142981 Valid Accuracy: 0.6713\n",
      "--------------------\n",
      "进行第12个epoch\n",
      "--------------------\n",
      "Epoch: 12 Train Loss: 0.721527 Train MAP: 0.5666312153615697 Train Accuracy: 0.6846\n",
      "Epoch: 12 Valid Loss: 0.760691 Valid MAP: 0.5622080482200718 Valid Accuracy: 0.6708\n",
      "--------------------\n",
      "进行第13个epoch\n",
      "--------------------\n",
      "Epoch: 13 Train Loss: 0.715904 Train MAP: 0.570298113553123 Train Accuracy: 0.6863\n",
      "Epoch: 13 Valid Loss: 0.758382 Valid MAP: 0.5660102665728287 Valid Accuracy: 0.6731\n",
      "--------------------\n",
      "进行第14个epoch\n",
      "--------------------\n",
      "Epoch: 14 Train Loss: 0.708913 Train MAP: 0.5761096116726117 Train Accuracy: 0.6894\n",
      "Epoch: 14 Valid Loss: 0.765390 Valid MAP: 0.5786475466976718 Valid Accuracy: 0.6733\n",
      "--------------------\n",
      "进行第15个epoch\n",
      "--------------------\n",
      "Epoch: 15 Train Loss: 0.703253 Train MAP: 0.5790883242287453 Train Accuracy: 0.6906\n",
      "Epoch: 15 Valid Loss: 0.755803 Valid MAP: 0.5642934840300409 Valid Accuracy: 0.6765\n",
      "--------------------\n",
      "进行第16个epoch\n",
      "--------------------\n",
      "Epoch: 16 Train Loss: 0.696248 Train MAP: 0.5875223227393664 Train Accuracy: 0.6945\n",
      "Epoch: 16 Valid Loss: 0.773204 Valid MAP: 0.5605152237758831 Valid Accuracy: 0.6735\n",
      "--------------------\n",
      "进行第17个epoch\n",
      "--------------------\n",
      "Epoch: 17 Train Loss: 0.689323 Train MAP: 0.5958171640359998 Train Accuracy: 0.6983\n",
      "Epoch: 17 Valid Loss: 0.766615 Valid MAP: 0.5722102550198379 Valid Accuracy: 0.6733\n",
      "--------------------\n",
      "进行第18个epoch\n",
      "--------------------\n",
      "Epoch: 18 Train Loss: 0.684641 Train MAP: 0.5984346863023489 Train Accuracy: 0.6995\n",
      "Epoch: 18 Valid Loss: 0.780151 Valid MAP: 0.5702309217679187 Valid Accuracy: 0.6734\n",
      "--------------------\n",
      "进行第19个epoch\n",
      "--------------------\n",
      "Epoch: 19 Train Loss: 0.679391 Train MAP: 0.6028673821361048 Train Accuracy: 0.7011\n",
      "Epoch: 19 Valid Loss: 0.781475 Valid MAP: 0.5686043366392372 Valid Accuracy: 0.6729\n",
      "--------------------\n",
      "进行第20个epoch\n",
      "--------------------\n",
      "Epoch: 20 Train Loss: 0.673482 Train MAP: 0.6091381952958227 Train Accuracy: 0.7051\n",
      "Epoch: 20 Valid Loss: 0.774741 Valid MAP: 0.5649093424192959 Valid Accuracy: 0.6767\n",
      "--------------------\n",
      "进行第21个epoch\n",
      "--------------------\n",
      "Epoch: 21 Train Loss: 0.666024 Train MAP: 0.6145650702552284 Train Accuracy: 0.7077\n",
      "Epoch: 21 Valid Loss: 0.773820 Valid MAP: 0.5733235997610444 Valid Accuracy: 0.6764\n",
      "--------------------\n",
      "进行第22个epoch\n",
      "--------------------\n",
      "Epoch: 22 Train Loss: 0.658823 Train MAP: 0.6198317559581142 Train Accuracy: 0.7111\n",
      "Epoch: 22 Valid Loss: 0.780580 Valid MAP: 0.5786024488690191 Valid Accuracy: 0.6740\n",
      "--------------------\n",
      "进行第23个epoch\n",
      "--------------------\n",
      "Epoch: 23 Train Loss: 0.655340 Train MAP: 0.6202250825245568 Train Accuracy: 0.7108\n",
      "Epoch: 23 Valid Loss: 0.789728 Valid MAP: 0.5777299397813217 Valid Accuracy: 0.6745\n",
      "--------------------\n",
      "进行第24个epoch\n",
      "--------------------\n",
      "Epoch: 24 Train Loss: 0.650890 Train MAP: 0.6267876643242898 Train Accuracy: 0.7147\n",
      "Epoch: 24 Valid Loss: 0.788741 Valid MAP: 0.5690019723850119 Valid Accuracy: 0.6736\n",
      "--------------------\n",
      "进行第25个epoch\n",
      "--------------------\n",
      "Epoch: 25 Train Loss: 0.647467 Train MAP: 0.629098039494867 Train Accuracy: 0.7158\n",
      "Epoch: 25 Valid Loss: 0.793896 Valid MAP: 0.5827708744223811 Valid Accuracy: 0.6698\n",
      "--------------------\n",
      "进行第26个epoch\n",
      "--------------------\n",
      "Epoch: 26 Train Loss: 0.639389 Train MAP: 0.635958706677798 Train Accuracy: 0.7192\n",
      "Epoch: 26 Valid Loss: 0.804653 Valid MAP: 0.573461515774671 Valid Accuracy: 0.6761\n",
      "--------------------\n",
      "进行第27个epoch\n",
      "--------------------\n",
      "Epoch: 27 Train Loss: 0.636679 Train MAP: 0.6375539964388754 Train Accuracy: 0.7204\n",
      "Epoch: 27 Valid Loss: 0.806946 Valid MAP: 0.5764500607724824 Valid Accuracy: 0.6729\n",
      "--------------------\n",
      "进行第28个epoch\n",
      "--------------------\n",
      "Epoch: 28 Train Loss: 0.630461 Train MAP: 0.6393829349549919 Train Accuracy: 0.7218\n",
      "Epoch: 28 Valid Loss: 0.807403 Valid MAP: 0.5754618925818072 Valid Accuracy: 0.6727\n",
      "--------------------\n",
      "进行第29个epoch\n",
      "--------------------\n",
      "Epoch: 29 Train Loss: 0.625762 Train MAP: 0.6453277565345589 Train Accuracy: 0.7251\n",
      "Epoch: 29 Valid Loss: 0.810712 Valid MAP: 0.5788377030441345 Valid Accuracy: 0.6693\n",
      "--------------------\n",
      "Train finished!\n",
      "Best Epoch: 20 Best Valid Loss:0.774741 Best Valid Accuracy: 0.6767\n"
     ]
    }
   ],
   "source": [
    "#训练开始\n",
    "best_valid_acc = 0.\n",
    "best_epoch = 0\n",
    "best_valid_loss = 0.\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    print(\"进行第{}个epoch\".format(epoch))\n",
    "    \n",
    "    train_loss, train_acc, train_MAP = train(model, Loss, optimizer, train_loader)\n",
    "    valid_loss, valid_acc, valid_MAP = evaluate(model, Loss, optimizer, valid_loader)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_epoch = epoch\n",
    "        best_valid_acc = valid_acc\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm-cnn-model-epoch30-embed128-lr0001-bz128-dp05-filter64-hz128.pth')\n",
    "        \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    print('-'*20)\n",
    "    print('Epoch:', epoch, 'Train Loss: %0.6f'% train_loss, 'Train MAP:',train_MAP , 'Train Accuracy: %0.4f'% train_acc)\n",
    "    print('Epoch:', epoch, 'Valid Loss: %0.6f'% valid_loss, 'Valid MAP:',valid_MAP , 'Valid Accuracy: %0.4f'% valid_acc)\n",
    "    print('-'*20)\n",
    "print(\"Train finished!\")\n",
    "print('Best Epoch:', best_epoch, 'Best Valid Loss:%0.6f'% best_valid_loss, 'Best Valid Accuracy: %0.4f'% best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(epoches)), train_loss_list, c = 'b', label = 'train loss')\n",
    "plt.plot(list(range(epoches)), valid_loss_list, c = 'r', label = 'valid loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-3b1f1e59954e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('zh')\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [vocab_dict[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    pred = model(tensor).argmax(dim = 1)\n",
    "    return pred.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm-cnn-model-epoch30-embed128-lr0001-bz128-dp05-filter64-hz128-06630.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_CNNModel(\n",
       "  (embed): Embedding(48074, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, num_layers=2, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 256), stride=(1, 1))\n",
       "  )\n",
       "  (f1): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = LSTMModel(vocab_size = VOCAB_SIZE, \n",
    "#                  embedding_size = EMBEDDING_SIZE,\n",
    "#                  output_size = OUTPUT_SIZE,\n",
    "#                  pad_idx = PAD_IDX,\n",
    "#                  num_layers = num_layers,\n",
    "#                  bidirectional = bidirectional,\n",
    "#                  hidden_size = hidden_size,\n",
    "#                  dropout = dropout)\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load('lstm-cnn-model-epoch60-embed128-lr001-bz128-dp05-filter100-06550.pth'))\n",
    "model.load_state_dict(torch.load(\"lstm-cnn-model-epoch30-embed128-lr0001-bz128-dp05-filter64-hz128.pth\"))\n",
    "model.eval()\n",
    "#predict_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6750, device='cuda:0')\n",
      "0.566357051445898\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_MAP = accuracy_computing(model(test_X), test_y.squeeze())\n",
    "print(test_acc)\n",
    "print(test_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6967731"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model参数\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6153472"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
